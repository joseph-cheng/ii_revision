\input{../../template.tex}

\begin{document}
\begin{enumerate}[label=(\alph*)]
  \item
    A 4KB cache means that the final 12 bits make up the cache tag, i.e. the last 3 characters of our hex addresses.

    \begin{enumerate}[label=(\roman*)]
      \item
        Numbering each memory access 1 to 10:

\begin{verbatim}
1: Miss (Line 0)
2: Miss (Line 1)
3: Miss (Line 0) (evicts cache line)
4: Hit  (Line 1)
5: Miss (Line 0) (evicts cache line)
6: Hit  (Line 1)
7: Hit  (Line 1)
8: Miss (Line 0) (evicts cache line)
9: Miss (Line 0) (evicts cache line)
10:Miss (Line 0) (evicts cache line)
\end{verbatim}

So a hit rate of 30\%

\item
\begin{verbatim}
1: Miss (Way 0)
2: Miss (Way 1)
3: Miss (Way 2)
4: Hit  (Way 1)
5: Miss (Way 3)
6: Hit  (Way 1)
7: Hit  (Way 1)
8: Hit  (Way 2)
9: Hit  (Way 0)
10:Hit  (Way 2)
\end{verbatim}

So a hit rate of 60\%

\item
\begin{verbatim}
1: Miss (Line 0, Way 0)
2: Miss (Line 1, Way 0)
3: Miss (Line 0, Way 1)
4: Hit  (Line 1, Way 0)
5: Miss (Line 0, Way 0) (evicts cache line)
6: Hit  (Line 1, Way 0)
7: Hit  (Line 1, Way 0)
8: Hit  (Line 0, Way 1)
9: Miss (Line 0, Way 0) (evicts cache line)
10:Hit  (Line 0, Way 1)
\end{verbatim}

So a hit rate of 50\%.
    \end{enumerate}

  \item

    A non-blocking cache  would bring performance benefits when combined with O3 execution because whilst a cache access is being served, we can continue to execute more instructions, and let us get cache hits whilst waiting for the miss to be served. We could also just overtake regular instructions past these memory access also, as long as they are independent.

    This means that we essentially reduce the cache miss penalty, since we waste less time waiting for the miss to be served.

    In our example from (a), every type of cache will miss on access 3, and hit on access 4. In a blocking cache, we have to wait for the miss to be served before we can send the request for access 4, even though these operations access different lines. In a non-blocking cache, however, we could use the hit-under-miss optimisation to have the 4th memory access by served (hit) whilst the miss on the 3rd memory access is still being processed.

    In order to implement this, we need extra state that keeps track of memory requests in progress, so we introduce a register called the Miss Status Holding Register that holds information like the address, format, destination register.

  \item
    If we have a load/store queue, then we might see certain behaviour in our queue (like multiple consecutive stores to the same address with no interleaving loads), which we can optimise away, since we are storing to a private cache, so no one will be able to read the in-between values that we store, and we will only be able to see the last one. This means that we can remove the in-between stores and just keep the last one, such that the cache does not see as many memory accesses.

    Furthermore, we can permit data-forwarding between certain instructions. If we see a write to an address, followed by a load from that same address, we can remove the load and just return the value that is being written, since we know exactly what data it will read.

  \item
    The following steps would occur, with C1 being the core in (a), and C2 being the core that is reading:

    C2 would broadcast a BusRd message on the bus for address \texttt{0x00001010}.

    C1 would snoop this message, and know that it needs to act, since it holds that line in the M state.

    C1 would allocate a block in the L2, and copy the line for \texttt{0x00001010} from its L1 into that L2 block, and put it onto the shared bus also with a Flush message.

    C1 would then move that line in its L1 into the S state.

    C2 would read this data from the bus, copy it into its L1, and move that line into the S state.




        
\end{enumerate}
\end{document}
