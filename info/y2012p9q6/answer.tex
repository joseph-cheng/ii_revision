\input{../../template.tex}

\begin{document}
\begin{enumerate}[label=(\alph*)]
  \item
    The probability of observing $N$ independent events is given by $\prod_{i=1}^N p_i$

    Therefore the information gained from observing these $N$ independent events is $\log_2({\prod_{i=1}^N p_i}) = \sum_{i=1}^N \log_2 p_i$

    The information gained from observing a particular $p_i$ is $\log_2 p_i$, so the sum of the information gained from observing each of these events is $\sum_{i=1}^N \log_2 p_i$, which as we have seen is the information gained from observing all $N$ independent events.

  \item
    We invoke the Source-Coding Theorem, which tells us that we can compress down to the entropy of the source.

    The entropy of the source is:

    $\frac{1}{2} + \frac{2}{4} + \frac{3}{8} + \frac{4}{16} + \frac{5}{32} + \frac{5}{32} = \frac{31}{16}$ bits, which is the shortest possible average code length of a decodable code for this source.

  \item
    \begin{enumerate}[label=(\roman*)]

      The capacity is given by:

      $C = W\log_2(1 + \frac{P}{N_0W})$

      Through the Shannon Hartley Theorem.

      By increasing $\frac{P}{N_0W}$ we are always able to increase the value of the $\log$, and thus the capacity. There is no limit.

    \item

      Again, we have the capacity given by:

      $C = W\log_2(1 + \frac{P}{N_0W})$

      By setting $x = \frac{P}{N_0W}$, we can rewrite this as:

      $C = \frac{P}{N_0x}\log_2(1 + x)$

      $C = \frac{P}{N_0}\log_2((1 + x)^{\frac{1}{x}})$

      As $W$ approaches infinity, $x$ approaches 0, and by substituting $y = \frac{1}{x}$, we see that $\lim_{x \to 0^+} \log_2((1 + x)^{\frac{1}{x}}) = \lim_{y \to \infty} \log_2((1 + \frac{1}{y})^y) = \log_2 e$, as a standard limit.

      Therefore, as $W$ approaches infinity, the capacity approaches $\frac{P}{N_0}\log_2 e$



    \end{enumerate}

  \item
    Not relevant.

  \item
    The Kolmogorov algorithmic complexity $K$ of a string of data is the length of the shortest program that can generate that string of data.

    We can consider writing a program that generates a string of data as a form of compression, so we expect that the Kolmogorov complexity $K$ and the Shannon entropy $H$ for a set of data to have the relation that $H \approx K$, or else we violate the Source-Coding theorem.

    The Kolmogorov complexity $K$ of a fractal would actually be very small, perhaps in the region of hundreds, since fractals can usually be generated by very simple programs.
        
\end{enumerate}
\end{document}
